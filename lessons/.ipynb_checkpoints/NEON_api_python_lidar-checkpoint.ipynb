{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download some lidar data using the [NEON Data Portal API](http://data.neonscience.org/data-api)\n",
    "\n",
    "First we are going to import `requests` and `json` libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "SITECODE = \"SRER\" #the site code for Santa Rita Experimental Range\n",
    "PRODUCTCODE = \"DP1.30003.001\" #the product code for Discrete Return lidar\n",
    "SERVER = \"http://data.neonscience.org/api/v0/\" #the current server address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to track how big the files sizes are - in case there is a breakdown in the transfer.\n",
    "\n",
    "Here is a simple way to track the file size using some definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "\n",
    "def file_size(file_path):\n",
    "    \"\"\"\n",
    "    this function will return the file size\n",
    "    \"\"\"\n",
    "    if os.path.isfile(file_path):\n",
    "        file_info = os.stat(file_path)\n",
    "        return convert_bytes(file_info.st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We query the server for the site location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site_response = requests.get(SERVER + 'sites/' + SITECODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a JSON blob for viewing the data at this site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_response_json = site_response.json()\n",
    "print(json.dumps(site_response_json, indent=2)) #using json.dumps for formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We subset the data to look for the discrete lidar data by date using the product code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_products = site_response_json['data']['dataProducts']\n",
    "\n",
    "#use a list comprehension here if you're feeling fancy\n",
    "for data_product in data_products:\n",
    "    if (data_product['dataProductCode'] == PRODUCTCODE):\n",
    "        months = data_product['availableMonths']\n",
    "\n",
    "print(months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the result we select the date range for only these lidar data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_response = requests.get(SERVER + 'data/' + PRODUCTCODE + '/' + SITECODE + '/' + '2017-08')\n",
    "data_response_json = data_response.json()\n",
    "print(json.dumps(data_response_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how many files are in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of files in dataset: \")\n",
    "number_files = print(len(data_response_json[\"data\"][\"files\"][0][\"url\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON blob returns with the name of the tiles and their url for downloading.\n",
    "\n",
    "We can go a level deeper into the JSON by looking for the individual file url, name, and size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = data_response_json[\"data\"][\"files\"][0][\"url\"]\n",
    "data_name = data_response_json[\"data\"][\"files\"][0][\"name\"]\n",
    "data_size = data_response_json[\"data\"][\"files\"][0][\"size\"]\n",
    "print(json.dumps(data_url, indent=0))\n",
    "print(json.dumps(data_name, indent=0))\n",
    "print(json.dumps(data_size, indent=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/vol_c/srer/' + data_name\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_response = requests.get(SERVER + 'data/' + PRODUCTCODE + '/' + SITECODE + '/' + '2017-08')\n",
    "data_response_json = data_response.json()\n",
    "data_url = data_response_json[\"data\"][\"files\"][0][\"url\"] \n",
    "print(\"Data URL: \" + data_url)\n",
    "data_name = data_response_json[\"data\"][\"files\"][0][\"name\"]\n",
    "data_size = data_response_json[\"data\"][\"files\"][0][\"size\"]\n",
    "path = '/vol_c/srer/' + data_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension_filename = data_url.split('/')[-1]\n",
    "local_filename = extension_filename.split('?')[-2]\n",
    "print(local_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to download individual files using the GET command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading file of size \" + data_size + \" to \" + path)\n",
    "response = requests.get(data_url, stream=True)  \n",
    "handle = open(path, \"wb\")\n",
    "start_time = time.time()\n",
    "for chunk in response.iter_content(chunk_size=67108864):\n",
    "    if chunk: # filter out to keep alive new chunks\n",
    "        handle.write(chunk)\n",
    "        print(\"Downloaded size: \" + file_size(path))\n",
    "    print(\"Expected file size: \" + data_size + \" bytes\")\n",
    "    print(\"Downloaded file size: \" + file_size(path))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've written a for loop to query the server for the JSON every time a new file is requested.\n",
    "\n",
    "Initially,  I had a problem with the loop timing out from the initial requests.get  because\n",
    "\n",
    "the NEON API creates a time stamp for when you hit the service, after about 5 minutes the files \n",
    "\n",
    "were breaking and only the headers were being downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, 443):\n",
    "    data_response = requests.get(SERVER + 'data/' + PRODUCTCODE + '/' + SITECODE + '/' + '2017-08')\n",
    "    data_response_json = data_response.json()\n",
    "    data_url = data_response_json[\"data\"][\"files\"][x][\"url\"] \n",
    "    print(\"Data URL: \" + data_url)\n",
    "    data_name = data_response_json[\"data\"][\"files\"][x][\"name\"]\n",
    "    data_size = data_response_json[\"data\"][\"files\"][x][\"size\"]\n",
    "    path = '/vol_c/srer/' + data_name\n",
    "    print(\"Downloading file of size \" + data_size + \" to \" + path)\n",
    "    response = requests.get(data_url)  \n",
    "    handle = open(path, \"wb\")\n",
    "    for chunk in response.iter_content(chunk_size=67108864):\n",
    "        if chunk: # filter out to keep alive new chunks\n",
    "            handle.write(chunk)\n",
    "    print(data_name + \" downloaded!\")\n",
    "    print(\"Expected file size: \" + data_size)\n",
    "    print(\"Downloaded file size: \" + file_size(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data are done downloading to my VM, I'm ready  to move onto the next step, filtering and processing the data with PDAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
